# -*- coding: utf-8 -*-
"""TIF_Grupo_N_LOS_hospital.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QdabYOxfPiVjtEfviT25kf2G3hGUBtNF
"""

import sys, platform, subprocess, json, os, math, random, re, warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from importlib.metadata import version, PackageNotFoundError

from google.colab import drive
from pathlib import Path

from sklearn.model_selection import GroupKFold, StratifiedKFold, GridSearchCV, RandomizedSearchCV
from sklearn.metrics import mean_absolute_error, mean_squared_error, roc_auc_score, average_precision_score, precision_recall_curve, roc_curve
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import ElasticNet, LogisticRegression
from sklearn.impute import SimpleImputer

from xgboost import XGBRegressor, XGBClassifier
from lightgbm import LGBMRegressor, LGBMClassifier

warnings.filterwarnings("ignore")

# Semillas globales
RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)
random.seed(RANDOM_STATE)

PERCENTIL_LONG_STAY = 0.80

GROUP_COL = "mrd_no"
DATE_ADM = "d_o_a"
DATE_DIS = "d_o_d"
TARGET_REG = "LOS_days"
TARGET_BIN = "long_stay"

drive.mount('/content/drive')
DATASET_CSV = "/content/drive/MyDrive/HDHI Admission data.csv"
df = pd.read_csv(DATASET_CSV)

print(df.shape)
df.head()

# 3) Funciones auxiliares
def normalizar_cols(cols):
    """Normaliza nombres de columnas a snake_case"""
    out = []
    for c in cols:
        c = c.strip()
        c = c.replace("\u200b","")
        c = re.sub(r"[^0-9A-Za-z]+", "_", c)
        c = re.sub(r"_+", "_", c).strip("_").lower()
        out.append(c)
    return out

def parsear_fechas(df, col, dayfirst=True):
    if col in df.columns:
        df[col] = pd.to_datetime(df[col], errors="coerce", dayfirst=dayfirst)
    return df

# Normalizar columnas
df.columns = normalizar_cols(df.columns)

# Parseo de fechas (hay formatos mixtos en el dataset)
parsear_fechas(df, "d_o_a", dayfirst=True)
parsear_fechas(df, "d_o_d", dayfirst=True)

print(df.head())

print("Nulos por columna:")
print(df.isna().mean().sort_values(ascending=False))

# Filtrar fechas inválidas (discharge < admission)
mask_bad = df[DATE_DIS] < df[DATE_ADM]
print("Registros con alta < ingreso:", mask_bad.sum())
df = df[~mask_bad].copy()

'''
quizas remover porque al imputar antes del split, hay fuga
# Tratamiento simple de nulos en numéricas (mediana) + bandera de ausencia
num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
num_cols = [c for c in num_cols if c not in []]  # excluir targets por ahora si existieran
for c in num_cols:
    print(c)
    if df[c].isna().any():
        df[c + "_isna"] = df[c].isna().astype(int)
        df[c] = df[c].fillna(df[c].median())
'''

print("Post-imputación, nulos:")
display(df.isna().mean().sort_values(ascending=False))

df[TARGET_REG] = (df[DATE_DIS] - df[DATE_ADM]).dt.days.astype(float)

p_long = PERCENTIL_LONG_STAY
thr = df[TARGET_REG].quantile(p_long)
df[TARGET_BIN] = (df[TARGET_REG] > thr).astype(int)

print("Estadísticos LOS:")
display(df[TARGET_REG].describe())
print("Umbral long-stay (percentil):", p_long, "→", thr)
df[TARGET_BIN].value_counts(normalize=True)

# Separa train/test sin fuga por paciente
train_idx, test_idx = next(GroupKFold(n_splits=5).split(df, groups=df[GROUP_COL]))
df_train = df.iloc[train_idx].copy()
df_test  = df.iloc[test_idx].copy()

print("Train:", df_train.shape, " Test:", df_test.shape)

# Filtrar filas sin LOS calculado
df_train = df_train.dropna(subset=[TARGET_REG]).copy()
df_test = df_test.dropna(subset=[TARGET_REG]).copy()

print("Train después del filtrado:", df_train.shape)
print("Test después del filtrado:", df_test.shape)

def subgroup_key(r):
  # Subgrupo simeple: tipo_admisión × bin_edad × ckd (Tolera NaN en age/admission_type/ckd)
  # admission_type
  adm = r.get("admission_type", "UNK")
  adm = "UNK" if pd.isna(adm) else str(adm)

  # age -> bin
  age = r.get("age", np.nan)
  if pd.isna(age):
      age_label = "NA"
  else:
      age_label = pd.cut([age],
                          bins=[0, 40, 60, 80, 120],
                          labels=["<=40", "41-60", "61-80", ">80"])[0]
      age_label = "NA" if pd.isna(age_label) else str(age_label)

  # ckd binario robusto
  raw_ckd = r.get("ckd", 0)
  ckd_flag = 0 if pd.isna(raw_ckd) else int(raw_ckd)

  return (adm, age_label, ckd_flag)


def recall_at_topk(y_true, y_score, k):
    y_true = np.asarray(y_true)
    y_score = np.asarray(y_score)
    k = int(k)
    if k <= 0:
        return 0.0
    # índices ordenados por score descendente
    top_idx = np.argsort(-y_score)[:k]
    tp_in_topk = y_true[top_idx].sum()
    total_pos = y_true.sum()
    return float(tp_in_topk / total_pos) if total_pos > 0 else 0.0

def baseline_regresion(train, test):
    """
    Baseline de regresión sin fuga:
    - Predice la mediana histórica de LOS por subgrupo (calculada SOLO en train).
    """
    keys_train = train.apply(subgroup_key, axis=1)
    grp_median = (
        pd.DataFrame({"key": keys_train, "los": train[TARGET_REG]})
        .groupby("key")["los"].median()
    )

    global_median = float(train[TARGET_REG].median())

    def predict_row(r):
        k = subgroup_key(r)
        return float(grp_median.get(k, global_median))

    y_pred = test.apply(predict_row, axis=1)
    mae = mean_absolute_error(test[TARGET_REG], y_pred)
    mse = mean_squared_error(test[TARGET_REG], y_pred)
    rmse = float(np.sqrt(mse))
    return {"mae": float(mae), "rmse": rmse}, y_pred

def baseline_clasificacion(train, test):
    """
    Baseline de clasificación SIN fuga:
    - 1) Define long_stay en TRAIN con un umbral por subgrupo (percentil).
    - 2) Calcula probabilidad histórica de long_stay por subgrupo en TRAIN.
    - 3) Predice en TEST esa probabilidad (score continuo) por subgrupo.
    Métricas AUC/AP se calculan con scores continuos.
    """
    # Umbral (percentil) por subgrupo para etiquetar en TRAIN
    keys_train = train.apply(subgroup_key, axis=1)
    thr_by_key = (
        pd.DataFrame({"key": keys_train, "los": train[TARGET_REG]})
        .groupby("key")["los"].quantile(PERCENTIL_LONG_STAY)
    )
    global_thr = float(train[TARGET_REG].quantile(PERCENTIL_LONG_STAY))

    # Etiqueta binaria en TRAIN (sin tocar TEST)
    train = train.copy()
    def label_row_train(r):
        k = subgroup_key(r)
        thr = float(thr_by_key.get(k, global_thr))
        return int(r[TARGET_REG] > thr)
    train[TARGET_BIN] = train.apply(label_row_train, axis=1)

    # Probabilidad histórica por subgrupo
    keys_train = train.apply(subgroup_key, axis=1)
    p_by_key = (
        pd.DataFrame({"key": keys_train, "y": train[TARGET_BIN]})
        .groupby("key")["y"].mean()
    )
    global_p = float(train[TARGET_BIN].mean())

    # Score continuo para TEST
    def score_row_test(r):
        k = subgroup_key(r)
        return float(p_by_key.get(k, global_p))
    y_score = test.apply(score_row_test, axis=1)

    # Métricas
    ap = float(average_precision_score(test[TARGET_BIN], y_score))
    roc = float(roc_auc_score(test[TARGET_BIN], y_score))
    k20 = max(1, int(0.2 * len(test)))
    rec_at_20 = float(recall_at_topk(test[TARGET_BIN], y_score, k=k20))
    return {"ap": ap, "roc_auc": roc, "recall_at_20": rec_at_20}, y_score

base_reg_metrics, _ = baseline_regresion(df_train, df_test)
base_cls_metrics, _ = baseline_clasificacion(df_train, df_test)

print("Baseline (Reg):", base_reg_metrics)
print("Baseline (Cls):", base_cls_metrics)

# Selección simple de columnas
num_features = [c for c in df.select_dtypes(include=[np.number]).columns if c not in [TARGET_REG, TARGET_BIN, GROUP_COL]]
cat_features = [c for c in df.columns if df[c].dtype in("object", "string")  and c not in [TARGET_REG, TARGET_BIN, GROUP_COL]]

numeric_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("onehot", OneHotEncoder(handle_unknown="ignore"))
])

preprocess = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, num_features),
        ("cat", categorical_transformer, cat_features)
    ]
)

print("Num features:", num_features)
print("Cat features:", cat_features)
