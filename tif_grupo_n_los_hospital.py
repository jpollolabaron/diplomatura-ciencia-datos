# -*- coding: utf-8 -*-
"""TIF_Grupo_N_LOS_hospital.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QdabYOxfPiVjtEfviT25kf2G3hGUBtNF
"""

import sys, platform, subprocess, json, os, math, random, re, warnings
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from importlib.metadata import version, PackageNotFoundError

from google.colab import drive
from pathlib import Path

from sklearn.model_selection import GroupKFold, StratifiedKFold, GridSearchCV, RandomizedSearchCV
from sklearn.metrics import mean_absolute_error, mean_squared_error, roc_auc_score, average_precision_score, precision_recall_curve, roc_curve
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.linear_model import ElasticNet, LogisticRegression
from sklearn.impute import SimpleImputer

from xgboost import XGBRegressor, XGBClassifier
from lightgbm import LGBMRegressor, LGBMClassifier

warnings.filterwarnings("ignore")

# Semillas globales
RANDOM_STATE = 42
np.random.seed(RANDOM_STATE)
random.seed(RANDOM_STATE)

PERCENTIL_LONG_STAY = 0.80

GROUP_COL = "mrd_no"
DATE_ADM = "d_o_a"
DATE_DIS = "d_o_d"
TARGET_REG = "LOS_days"
TARGET_BIN = "long_stay"

drive.mount('/content/drive')
DATASET_CSV = "/content/drive/MyDrive/HDHI Admission data.csv"
df = pd.read_csv(DATASET_CSV)

print(df.shape)
df.head()

# 3) Funciones auxiliares
def normalizar_cols(cols):
    """Normaliza nombres de columnas a snake_case"""
    out = []
    for c in cols:
        c = c.strip()
        c = c.replace("\u200b","")
        c = re.sub(r"[^0-9A-Za-z]+", "_", c)
        c = re.sub(r"_+", "_", c).strip("_").lower()
        out.append(c)
    return out

def parsear_fechas(df, col, dayfirst=True):
    if col in df.columns:
        df[col] = pd.to_datetime(df[col], errors="coerce", dayfirst=dayfirst)
    return df

# Normalizar columnas
df.columns = normalizar_cols(df.columns)

# Parseo de fechas (hay formatos mixtos en el dataset)
parsear_fechas(df, "d_o_a", dayfirst=True)
parsear_fechas(df, "d_o_d", dayfirst=True)

print(df.head())

print("Nulos por columna:")
print(df.isna().mean().sort_values(ascending=False))

# Filtrar fechas inválidas (discharge < admission)
mask_bad = df[DATE_DIS] < df[DATE_ADM]
print("Registros con alta < ingreso:", mask_bad.sum())
df = df[~mask_bad].copy()

'''
quizas remover porque al imputar antes del split, hay fuga
# Tratamiento simple de nulos en numéricas (mediana) + bandera de ausencia
num_cols = df.select_dtypes(include=[np.number]).columns.tolist()
num_cols = [c for c in num_cols if c not in []]  # excluir targets por ahora si existieran
for c in num_cols:
    print(c)
    if df[c].isna().any():
        df[c + "_isna"] = df[c].isna().astype(int)
        df[c] = df[c].fillna(df[c].median())
'''

print("Post-imputación, nulos:")
print(df.isna().mean().sort_values(ascending=False))

# ------------------------------------------------------------------
# Crear TARGET_REG (LOS_days) ANTES de split y filtrar nulos de LOS
# ------------------------------------------------------------------
df[TARGET_REG] = (df[DATE_DIS] - df[DATE_ADM]).dt.days.astype(float)
df = df.dropna(subset=[TARGET_REG]).copy()

# Separa train/test sin fuga por paciente
if df[GROUP_COL].isna().any():
    df[GROUP_COL] = df[GROUP_COL].astype("string").fillna("missing_id")

train_idx, test_idx = next(GroupKFold(n_splits=5).split(df, groups=df[GROUP_COL]))

df_train = df.iloc[train_idx].copy()
df_test  = df.iloc[test_idx].copy()

print("Train:", df_train.shape, " Test:", df_test.shape)

print("Train después del filtrado:", df_train.shape)
print("Test después del filtrado:", df_test.shape)

def subgroup_key(r):
  # Subgrupo simeple: tipo_admisión × bin_edad × ckd (Tolera NaN en age/admission_type/ckd)
  # Campo de admisión tras normalizar: "type_of_admission_er_opd"
  adm = r.get("type_of_admission_er_opd", r.get("admission_type", "UNK"))
  adm = "UNK" if pd.isna(adm) else str(adm)

  # age -> bin
  age = r.get("age", np.nan)
  if pd.isna(age):
      age_label = "NA"
  else:
      age_label = pd.cut([age],
                          bins=[0, 40, 60, 80, 120],
                          labels=["<=40", "41-60", "61-80", ">80"])[0]
      age_label = "NA" if pd.isna(age_label) else str(age_label)

  # ckd binario robusto
  raw_ckd = r.get("ckd", 0)
  ckd_flag = 0 if pd.isna(raw_ckd) else int(raw_ckd)

  return (adm, age_label, ckd_flag)


def recall_at_topk(y_true, y_score, k):
    y_true = np.asarray(y_true)
    y_score = np.asarray(y_score)
    k = int(k)
    if k <= 0:
        return 0.0
    # índices ordenados por score descendente
    top_idx = np.argsort(-y_score)[:k]
    tp_in_topk = y_true[top_idx].sum()
    total_pos = y_true.sum()
    return float(tp_in_topk / total_pos) if total_pos > 0 else 0.0

def baseline_regresion(train, test):
    """
    Baseline de regresión sin fuga:
    - Predice la mediana histórica de LOS por subgrupo (calculada SOLO en train).
    """
    keys_train = train.apply(subgroup_key, axis=1)
    grp_median = (
        pd.DataFrame({"key": keys_train, "los": train[TARGET_REG]})
        .groupby("key")["los"].median()
    )

    global_median = float(train[TARGET_REG].median())

    def predict_row(r):
        k = subgroup_key(r)
        return float(grp_median.get(k, global_median))

    y_pred = test.apply(predict_row, axis=1)
    mae = mean_absolute_error(test[TARGET_REG], y_pred)
    mse = mean_squared_error(test[TARGET_REG], y_pred)
    rmse = float(np.sqrt(mse))
    return {"mae": float(mae), "rmse": rmse}, y_pred

def baseline_clasificacion(train, test):
    """
    Baseline de clasificación SIN fuga:
    - 1) Define long_stay en TRAIN con un umbral por subgrupo (percentil).
    - 2) Calcula probabilidad histórica de long_stay por subgrupo en TRAIN.
    - 3) Predice en TEST esa probabilidad (score continuo) por subgrupo.
    Métricas AUC/AP se calculan con scores continuos.
    """
    # Umbral (percentil) por subgrupo para etiquetar en TRAIN
    keys_train = train.apply(subgroup_key, axis=1)
    thr_by_key = (
        pd.DataFrame({"key": keys_train, "los": train[TARGET_REG]})
        .groupby("key")["los"].quantile(PERCENTIL_LONG_STAY)
    )
    global_thr = float(train[TARGET_REG].quantile(PERCENTIL_LONG_STAY))

    # Etiqueta binaria en TRAIN (sin tocar TEST)
    train = train.copy()
    def label_row_train(r):
        k = subgroup_key(r)
        thr = float(thr_by_key.get(k, global_thr))
        return int(r[TARGET_REG] > thr)
    train[TARGET_BIN] = train.apply(label_row_train, axis=1)

    # Probabilidad histórica por subgrupo
    keys_train = train.apply(subgroup_key, axis=1)
    p_by_key = (
        pd.DataFrame({"key": keys_train, "y": train[TARGET_BIN]})
        .groupby("key")["y"].mean()
    )
    global_p = float(train[TARGET_BIN].mean())

    # Score continuo para TEST
    def score_row_test(r):
        k = subgroup_key(r)
        return float(p_by_key.get(k, global_p))
    y_score = test.apply(score_row_test, axis=1)

    # Métricas
    ap = float(average_precision_score(test[TARGET_BIN], y_score))
    roc = float(roc_auc_score(test[TARGET_BIN], y_score))
    k20 = max(1, int(0.2 * len(test)))
    rec_at_20 = float(recall_at_topk(test[TARGET_BIN], y_score, k=k20))
    return {"ap": ap, "roc_auc": roc, "recall_at_20": rec_at_20}, y_score

def _label_with_train_thresholds(r):
    k = subgroup_key(r)
    thr = float(thr_by_key.get(k, global_thr))
    return int(r[TARGET_REG] > thr)

# Definir TARGET_BIN consistente con el baseline: umbral por SUBGRUPO aprendido en TRAIN
p_long = PERCENTIL_LONG_STAY
keys_train_for_thr = df_train.apply(subgroup_key, axis=1)
thr_by_key = (
    pd.DataFrame({"key": keys_train_for_thr, "los": df_train[TARGET_REG]})
      .groupby("key")["los"].quantile(p_long)
)
global_thr = float(df_train[TARGET_REG].quantile(p_long))

df_train[TARGET_BIN] = df_train.apply(_label_with_train_thresholds, axis=1)
df_test[TARGET_BIN]  = df_test.apply(_label_with_train_thresholds, axis=1)

print("Post-imputación, nulos:")
print(df.isna().mean().sort_values(ascending=False))

print("Estadísticos LOS:")
print(df[TARGET_REG].describe())

print("Umbral long-stay (percentil global en TRAIN):", p_long, "→", global_thr)
print("Prevalencia long_stay - train/test:", df_train[TARGET_BIN].mean(), "/", df_test[TARGET_BIN].mean())

base_reg_metrics, _ = baseline_regresion(df_train, df_test)
base_cls_metrics, _ = baseline_clasificacion(df_train, df_test)

print("Baseline (Reg):", base_reg_metrics)
print("Baseline (Cls):", base_cls_metrics)

# Selección simple de columnas
num_features = [c for c in df.select_dtypes(include=[np.number]).columns
                if c not in [TARGET_REG, TARGET_BIN, GROUP_COL]]

cat_features = [c for c in df.select_dtypes(include=["object", "string"]).columns
                if c not in [TARGET_REG, TARGET_BIN, GROUP_COL]]

numeric_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("onehot", OneHotEncoder(handle_unknown="ignore"))
])

preprocess = ColumnTransformer(
    transformers=[
        ("num", numeric_transformer, num_features),
        ("cat", categorical_transformer, cat_features)
    ]
)

print("Num features:", num_features)
print("Cat features:", cat_features)

#Modelado: Regresión (LOS)

from sklearn.model_selection import GroupKFold, GridSearchCV

X_train = df_train.drop([TARGET_REG, TARGET_BIN], axis=1)
y_train_reg = df_train[TARGET_REG]
X_test  = df_test.drop([TARGET_REG, TARGET_BIN], axis=1)
y_test_reg = df_test[TARGET_REG]
groups = df_train[GROUP_COL]

# Elastic Net
enet = Pipeline(steps=[
    ("preprocess", preprocess),
    ("model", ElasticNet(random_state=RANDOM_STATE, max_iter=10000))
])


param_enet = {
    "model__alpha": [0.1, 1.0],
    "model__l1_ratio": [0.5]
}

'''
#este tarda mas
param_enet = {
    "model__alpha": [0.01, 0.1, 1.0],
    "model__l1_ratio": [0.1, 0.5, 0.9]
}

'''

cv = GroupKFold(n_splits=5)
gs_enet = GridSearchCV(enet, param_enet, scoring="neg_mean_absolute_error",
                       cv=cv.split(X_train, y_train_reg, groups=groups), n_jobs=-1)
gs_enet.fit(X_train, y_train_reg)

def eval_regressor(pipeline, X_te, y_te):
    pred = pipeline.predict(X_te)
    mae = mean_absolute_error(y_te, pred)
    mse = mean_squared_error(y_te, pred)
    rmse = np.sqrt(mse)          # raíz cuadrada del MSE
    return {"mae": float(mae), "rmse": float(rmse)}, pred

enet_metrics, enet_pred = eval_regressor(gs_enet.best_estimator_, X_test, y_test_reg)
print("ElasticNet (test):", enet_metrics)

# XGB/LGBM opcional (si están instalados)
gbm_results = {}

xgb_reg = Pipeline(steps=[
    ("preprocess", preprocess),
    ("model", XGBRegressor(
        random_state=RANDOM_STATE, n_estimators=100, learning_rate=0.05,
        subsample=0.8, colsample_bytree=0.8, max_depth=4, reg_lambda=1.0
    ))
])
xgb_reg.fit(X_train, y_train_reg)
gbm_results["xgb"], xgb_pred = eval_regressor(xgb_reg, X_test, y_test_reg)
print("XGBRegressor (test):", gbm_results["xgb"])

lgb_reg = Pipeline(steps=[
    ("preprocess", preprocess),
    ("model", LGBMRegressor(
        random_state=RANDOM_STATE, n_estimators=150, learning_rate=0.05,
        subsample=0.8, colsample_bytree=0.8, max_depth=-1, reg_lambda=0.0
    ))
])
lgb_reg.fit(X_train, y_train_reg)
gbm_results["lgbm"], lgbm_pred = eval_regressor(lgb_reg, X_test, y_test_reg)
print("LGBMRegressor (test):", gbm_results["lgbm"])

# Modelado: Clasificación (Long-stay)

y_train_bin = df_train[TARGET_BIN]
y_test_bin = df_test[TARGET_BIN]

logreg = Pipeline(steps=[
    ("preprocess", preprocess),
    ("model", LogisticRegression(max_iter=1000, random_state=RANDOM_STATE))
])

param_lr = {
    "model__C": [0.1, 1.0, 3.0],
    "model__penalty": ["l2"]
}

cv = GroupKFold(n_splits=5)
gs_lr = GridSearchCV(logreg, param_lr, scoring="average_precision",
                     cv=cv.split(X_train, y_train_bin, groups=groups), n_jobs=-1)
gs_lr.fit(X_train, y_train_bin)

def eval_classifier(pipeline, X_te, y_te):
    proba = pipeline.predict_proba(X_te)[:,1]
    ap = average_precision_score(y_te, proba)
    roc = roc_auc_score(y_te, proba)
    # Recall@Top-20%
    k = max(1, int(0.2 * len(y_te)))
    order = np.argsort(-proba)[:k]
    recall_at_20 = y_te.iloc[order].sum() / y_te.sum() if y_te.sum() > 0 else 0.0
    return {"ap": float(ap), "roc_auc": float(roc), "recall_at_20": float(recall_at_20)}, proba

lr_metrics, lr_proba = eval_classifier(gs_lr.best_estimator_, X_test, y_test_bin)
print("Logistic Regression (test):", lr_metrics)

xgb_cls_metrics = None

xgb_cls = Pipeline(steps=[
    ("preprocess", preprocess),
    ("model", XGBClassifier(
        random_state=RANDOM_STATE, n_estimators=150, learning_rate=0.05,
        subsample=0.8, colsample_bytree=0.8, max_depth=4, reg_lambda=1.0,
        eval_metric="logloss"
    ))
])
xgb_cls.fit(X_train, y_train_bin)
xgb_cls_metrics, xgb_proba = eval_classifier(xgb_cls, X_test, y_test_bin)
print("XGBClassifier (test):", xgb_cls_metrics)

#Selección de modelo y gráficas (PR/ROC, comparación vs baseline)

def plot_pr(y_true, scores, title):
    precision, recall, _ = precision_recall_curve(y_true, scores)
    plt.figure()
    plt.step(recall, precision, where='post')
    plt.xlabel("Recall")
    plt.ylabel("Precision")
    plt.title(title)
    plt.grid(True)
    plt.show()

def plot_roc(y_true, scores, title):
    fpr, tpr, _ = roc_curve(y_true, scores)
    plt.figure()
    plt.plot(fpr, tpr)
    plt.plot([0,1],[0,1], linestyle="--")
    plt.xlabel("FPR")
    plt.ylabel("TPR")
    plt.title(title)
    plt.grid(True)
    plt.show()

# PR/ROC para el mejor clasificador disponible (LogReg por defecto)
plot_pr(y_test_bin, lr_proba, "Precision-Recall — Logistic Regression")
plot_roc(y_test_bin, lr_proba, "ROC — Logistic Regression")

# Tabla comparativa simple
comp = {
    "baseline_reg": base_reg_metrics,
    "enet_reg": enet_metrics
}
if 'gbm_results' in globals():
    for k,v in gbm_results.items():
        comp[f"{k}_reg"] = v

comp_cls = {
    "baseline_cls": base_cls_metrics,
    "logreg_cls": lr_metrics
}
if 'xgb_cls_metrics' in globals() and xgb_cls_metrics is not None:
    comp_cls["xgb_cls"] = xgb_cls_metrics

print("\nComparación Regresión:")
print(json.dumps(comp, indent=2))
print("\nComparación Clasificación:")
print(json.dumps(comp_cls, indent=2))
